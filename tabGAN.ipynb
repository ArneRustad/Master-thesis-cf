{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7abd9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableGAN:\n",
    "    \"\"\"\n",
    "    Class for creating a tabular GAN that can also generate counterfactual explanations through a post-processing step.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, dim_latent = 128, dim_hidden = 256, batch_size = 500, gumbel_temperature = 0.5,\n",
    "                n_critic = 5, wgan_lambda = 10, adam_lr = 0.0002, adam_beta1 = 0, adam_beta2 = 0.999,\n",
    "                 ckpt_dir = None, ckpt_every = None,\n",
    "                ckpt_max_to_keep = None, ckpt_name = \"ckpt_epoch\", noise_discrete_unif_max = 0,\n",
    "                 quantile_transformation_int = False, n_quantiles_int = 1000, quantile_rand_transformation = True,\n",
    "                 qtr_fraction = 0.4, qtr_apply_lbound = 0.05, use_query = True):\n",
    "        #Initialize variables\n",
    "        self.data = data\n",
    "        self.columns = data.columns\n",
    "        self.n_columns = len(self.columns)\n",
    "        self.nrow = data.shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.dim_latent = dim_latent\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.gumbel_temperature = gumbel_temperature\n",
    "        self.adam_lr = adam_lr\n",
    "        self.adam_beta1 = adam_beta1\n",
    "        self.n_critic = n_critic\n",
    "        self.wgan_lambda = wgan_lambda\n",
    "        self.adam_lr = adam_lr\n",
    "        self.adam_beta1 = adam_beta1\n",
    "        self.adam_beta2 = adam_beta2\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "        self.ckpt_every = ckpt_every\n",
    "        self.ckpt_max_to_keep = ckpt_max_to_keep\n",
    "        self.ckpt_name = ckpt_name\n",
    "        self.ckpt_prefix = os.path.join(self.ckpt_dir, self.ckpt_name) if not self.ckpt_dir is None else None\n",
    "        self.noise_discrete_unif_max = noise_discrete_unif_max\n",
    "        self.quantile_transformation_int = quantile_transformation_int\n",
    "        self.quantile_rand_transformation = quantile_rand_transformation\n",
    "        self.n_quantiles_int = n_quantiles_int\n",
    "        self.uninitialized_opt_vars = True\n",
    "        self.qtr_fraction = qtr_fraction\n",
    "        self.qtr_apply_lbound = qtr_apply_lbound\n",
    "        self.use_query = use_query\n",
    "\n",
    "        # Separate numeric data, fit numeric scaler and scale numeric data. Store numeric column names.\n",
    "        self.data_num = data.select_dtypes(include=np.number)\n",
    "        self.columns_num = self.data_num.columns\n",
    "        self.n_columns_num = len(self.data_num.columns)\n",
    "        self.columns_num_int_mask = self.data_num.dtypes.astype(str).str.contains(\"int\")\n",
    "        self.columns_int = self.columns_num[self.columns_num_int_mask]\n",
    "        self.columns_num_int_pos = np.arange(len(self.columns_num))[self.columns_num_int_mask]\n",
    "        self.columns_float = self.columns_num[np.logical_not(self.columns_num_int_mask)]\n",
    "        if self.quantile_transformation_int:\n",
    "            self.scaler_num = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"float\", StandardScaler(), self.columns_float),\n",
    "                    (\"int\", QuantileTransformer(n_quantiles=n_quantiles_int, output_distribution='normal'),\n",
    "                                                self.columns_int)\n",
    "                ]\n",
    "            )  \n",
    "            self.data_num_scaled = self.scaler_num.fit_transform(self.data_num)\n",
    "            \n",
    "            if self.quantile_rand_transformation:\n",
    "                 self.data_num_scaled = self.randomize_quantile_transformation(self.data_num_scaled)\n",
    "        else:\n",
    "            self.scaler_num = StandardScaler()\n",
    "            self.data_num_scaled = self.scaler_num.fit_transform(self.data_num)\n",
    "            self.columns_num_int_mask = None\n",
    "            self.columns_int = None\n",
    "            self.columns_float = None\n",
    "            self.columns_num_int_pos = None\n",
    "        \n",
    "        # Separate discrete data, fit one-hot-encoder, perform one hot encoding. Store discrete column names\n",
    "        # and store the number of categories for each discrete variable\n",
    "        self.data_discrete = data.select_dtypes(exclude = np.number)\n",
    "        self.columns_discrete = self.data_discrete.columns\n",
    "        self.n_columns_discrete = len(self.columns_discrete)\n",
    "        \n",
    "        self.oh_encoder = OneHotEncoder(sparse = False)\n",
    "        self.data_discrete_oh = self.oh_encoder.fit_transform(self.data_discrete)\n",
    "        self.n_columns_discrete_oh = self.data_discrete_oh.shape[1]\n",
    "        #if (self.noise_discrete_unif_max > 0):\n",
    "            #noise_discrete = np.random.uniform(low = 0, high = self.noise_discrete_unif_max,\n",
    "            #                                   size = self.data_discrete_oh.shape)\n",
    "            #self.data_discrete_oh += noise_discrete * np.where(self.data_discrete_oh > 0.5, -1, 1)\n",
    "        \n",
    "        self.categories_len = [len(i) for i in self.oh_encoder.categories_]\n",
    "        \n",
    "        #Create Gumbel-activation function\n",
    "        tf.keras.utils.get_custom_objects().update({'gumbel_softmax': Activation(self.gumbel_softmax)})\n",
    "        \n",
    "        # Create generator and discriminator objects as well as discriminator and generator optimizer\n",
    "        self.initialize_gan()\n",
    "        # If needed create checkpoint manager\n",
    "        if (self.ckpt_dir != None):\n",
    "            self.initialize_cptk()\n",
    "    \n",
    "    def initialize_gan(self):\n",
    "        \"\"\"\n",
    "        Internal function used for initializing the GAN architecture\n",
    "        \"\"\"\n",
    "        # Create generator and discriminator objects\n",
    "        self.generator = self.create_generator()\n",
    "        self.discriminator = self.create_discriminator()\n",
    "        \n",
    "        # Create optimizers for generator and discriminator\n",
    "        self.generator_optimizer = tf.keras.optimizers.Adam(learning_rate = self.adam_lr, beta_1 = self.adam_beta1,\n",
    "                                                           beta_2 = self.adam_beta2)\n",
    "        self.discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate = self.adam_lr, beta_1 = self.adam_beta1,\n",
    "                                                               beta_2 = self.adam_beta2)\n",
    "        self.start_epoch = 0\n",
    "        \n",
    "        self.train_step = tf.function(self.train_step_func)\n",
    "        \n",
    "    \n",
    "    def randomize_quantile_transformation(self, data):\n",
    "        \"\"\"\n",
    "        Internal function for performing the randomized quantile transformation\n",
    "        \"\"\"\n",
    "        qt_transformer = self.scaler_num.named_transformers_[\"int\"]\n",
    "        references = np.copy(qt_transformer.references_)\n",
    "        quantiles = np.copy(qt_transformer.quantiles_)\n",
    "        lower_bound_references = 1e-7\n",
    "        references[[0,-1]] = [lower_bound_references, 1 - lower_bound_references]\n",
    "        for i, col in enumerate(self.columns_num_int_pos):\n",
    "            quantiles_curr = quantiles[:,i]\n",
    "            quantiles_unique_integer = np.unique(quantiles_curr)\n",
    "            quantiles_unique_integer = quantiles_unique_integer[np.isclose(np.mod(quantiles_unique_integer, 1), 0)]\n",
    "            for integer in quantiles_unique_integer:\n",
    "                curr_references = references[np.isclose(quantiles_curr,integer)]\n",
    "                n_curr_references = curr_references.shape[0]\n",
    "                if (n_curr_references >= self.qtr_apply_lbound * self.n_quantiles_int):\n",
    "                    mask = self.data_num[self.columns_int[i]] == integer\n",
    "                    n_obs_curr = np.sum(mask)\n",
    "                    curr_reference_range = curr_references[-1] - curr_references[0]\n",
    "                    low = curr_references[0] + curr_reference_range * (0.5 - self.qtr_fraction / 2)\n",
    "                    high = curr_references[0] + curr_reference_range * (0.5 + self.qtr_fraction / 2)\n",
    "                    data[mask, col] = scipy.stats.norm.ppf(np.random.uniform(low = low, high = high, size = n_obs_curr))           \n",
    "        return data\n",
    "    \n",
    "    def initialize_cptk(self):\n",
    "        \"\"\"\n",
    "        Internal function for initializing checkpoint mangager used to save the progress of the model.\n",
    "        \"\"\"\n",
    "        os.makedirs(self.ckpt_dir, exist_ok = True)\n",
    "        self.ckpt = tf.train.Checkpoint(epoch = tf.Variable(0),\n",
    "                                         generator_opt=self.generator_optimizer,\n",
    "                                         discriminator_opt=self.discriminator_optimizer,\n",
    "                                         generator=self.generator,\n",
    "                                         discriminator=self.discriminator)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(self.ckpt, self.ckpt_dir,\n",
    "                                                   max_to_keep = self.ckpt_max_to_keep,\n",
    "                                                  checkpoint_name = self.ckpt_name)\n",
    "    \n",
    "    def inv_data_transform(self, data_num_scaled, data_discrete_oh):\n",
    "        \"\"\"\n",
    "        Internal function used for inverting the data transformation done in preprocessing\n",
    "        \"\"\"\n",
    "        data_discrete = pd.DataFrame(self.oh_encoder.inverse_transform(data_discrete_oh), columns = self.columns_discrete)\n",
    "        if (self.quantile_transformation_int):\n",
    "            if (len(self.columns_float) > 0):\n",
    "                data_float = pd.DataFrame(\n",
    "                    self.scaler_num.named_transformers_[\"float\"].inverse_transform(data_num_scaled[:,np.logical_not(self.columns_num_int_mask)]),\n",
    "                    columns = self.columns_float\n",
    "                )\n",
    "            else:\n",
    "                data_float = None\n",
    "            \n",
    "            if (len(self.columns_int) > 0):\n",
    "                data_int_scaled = pd.DataFrame(data_num_scaled[:, self.columns_num_int_mask], columns=self.columns_int)\n",
    "                data_int = pd.DataFrame(\n",
    "                    self.scaler_num.named_transformers_[\"int\"].inverse_transform(data_int_scaled),\n",
    "                    columns = self.columns_int\n",
    "                )\n",
    "            else:\n",
    "                data_int = None\n",
    "        else:\n",
    "            data_float = pd.DataFrame(self.scaler_num.inverse_transform(data_num_scaled), columns = self.columns_num)\n",
    "            data_int = None\n",
    "        return(pd.concat([data_float, data_int, data_discrete], axis = 1)[self.columns])\n",
    "    \n",
    "    def generate_queries(self, n):\n",
    "        \"\"\"\n",
    "        Currently a dummy function for generating queries. Query functionality is not yet implemented\n",
    "        \"\"\"\n",
    "        return(tf.zeros([n, self.n_columns_discrete_oh]))\n",
    "    \n",
    "    def generate_data(self, n = None):\n",
    "        \"\"\"\n",
    "        Function for generating data used the data synthesizer\n",
    "        \"\"\"\n",
    "        if (n == None):\n",
    "            n = self.nrow\n",
    "        noise = self.generate_latent(n)\n",
    "        queries = self.generate_queries(n)\n",
    "        gen_data_num_scaled, gen_data_discrete_oh = self.generator.predict([noise, queries])\n",
    "        return(self.inv_data_transform(gen_data_num_scaled, gen_data_discrete_oh))\n",
    "    \n",
    "    def generate_data_scaled(self, n = None):\n",
    "        \"\"\"\n",
    "        Function for generating data directly from the generator without inverting any transformations. Mostly used for debugging.\n",
    "        \"\"\"\n",
    "        if (n == None):\n",
    "            n = self.nrow\n",
    "        noise = self.generate_latent(n)\n",
    "        queries = self.generate_queries(n)\n",
    "        gen_data_num_scaled, gen_data_discrete_oh = self.generator.predict([noise, queries])\n",
    "        columns_discrete_oh = []\n",
    "        for i, col in enumerate(self.columns_discrete):\n",
    "            for category in self.oh_encoder.categories_[i]:\n",
    "                columns_discrete_oh.append(col + \":\" + category)\n",
    "        return pd.concat((pd.DataFrame(gen_data_num_scaled, columns = self.columns_num),\n",
    "                          pd.DataFrame(gen_data_discrete_oh, columns = columns_discrete_oh)),\n",
    "                         axis=1\n",
    "                        )\n",
    "    \n",
    "    def create_discriminator(self):\n",
    "        \"\"\"\n",
    "        Internal function for creating the critic neural network.\n",
    "        \"\"\"\n",
    "        input_numeric = Input(shape=(self.n_columns_num), name=\"Numeric_input\")\n",
    "        input_discrete = Input(shape=(self.n_columns_discrete_oh), name = \"Discrete_input\")\n",
    "        if self.use_query:\n",
    "            query = Input(shape=(self.n_columns_discrete_oh), name = \"Query\")\n",
    "            combined1 = concatenate([input_numeric, input_discrete, query], name=\"Combining_input\")\n",
    "            inputs = [input_numeric, input_discrete, query]\n",
    "        else:\n",
    "            combined1 = concatenate([input_numeric, input_discrete], name=\"Combining_input\")\n",
    "            inputs = [input_numeric, input_discrete]\n",
    "        hidden1 = Dense(self.dim_hidden, activation= LeakyReLU(), name=\"hidden1\")(combined1)\n",
    "        hidden2 = Dense(self.dim_hidden, activation = LeakyReLU(), name=\"hidden2\")(hidden1)\n",
    "        output = Dense(1, name=\"output_discriminator\")(hidden2)\n",
    "        model = Model(inputs=inputs, outputs=output)\n",
    "        return(model)\n",
    "\n",
    "    def create_generator(self):\n",
    "        \"\"\"\n",
    "        Internal function for creating the generator neural network\n",
    "        \"\"\"\n",
    "        latent = Input(shape=(self.dim_latent), name=\"Latent\")\n",
    "        if self.use_query:\n",
    "            query = Input(shape=(self.n_columns_discrete_oh), name=\"Query\")\n",
    "            combined1 = concatenate([latent, query], name=\"Concatenate_input\")\n",
    "            inputs = [latent, query]\n",
    "        else:\n",
    "            combined1 = latent\n",
    "            inputs = [latent]\n",
    "        hidden1 = Dense(self.dim_hidden, activation = LeakyReLU(), name=\"Hidden1\")(combined1)\n",
    "        hidden2 = Dense(self.dim_hidden, activation = LeakyReLU(), name=\"Hidden2\")(hidden1)\n",
    "        \n",
    "        if (self.n_columns_discrete == 0):\n",
    "            raise Exception(\"tableGAN not yet implemented for zero discrete columns\")\n",
    "        elif(self.n_columns_discrete == 1):\n",
    "            output_discrete_i = Dense(self.categories_len[0],\n",
    "                                      name=\"%s_output\" % self.columns_discrete[0])(hidden2)\n",
    "            output_discrete = Activation(\"gumbel_softmax\", name = \"Gumbel_softmax\")(output_discrete_i)\n",
    "        else:\n",
    "            output_discrete_sep = []\n",
    "            for i in range(self.n_columns_discrete):\n",
    "                output_discrete_i = Dense(self.categories_len[i],\n",
    "                                          name=\"%s_output\" % self.columns_discrete[i])(hidden2)\n",
    "                output_discrete_sep.append(Activation(\"gumbel_softmax\", name = \"Gumbel_softmax%d\" % (i+1))(output_discrete_i))\n",
    "\n",
    "            output_discrete = concatenate(output_discrete_sep, name=\"Discrete_output\")\n",
    "        \n",
    "        output_numeric = Dense(self.n_columns_num, name=\"Numeric_output\")(hidden2)\n",
    "        model = Model(inputs=inputs, outputs=[output_numeric, output_discrete])\n",
    "        return(model)\n",
    "    \n",
    "    def generate_latent(self, n):\n",
    "        \"\"\"\n",
    "        Internal function for generating latent noise as input for generator \n",
    "        \"\"\"\n",
    "        return(tf.random.normal([n, self.dim_latent]))\n",
    "    \n",
    "    def gumbel_softmax(self, logits):\n",
    "        \"\"\"\n",
    "        Internal function for used in creating of gumbel softmax layers\n",
    "        \"\"\"\n",
    "        return(tfd.RelaxedOneHotCategorical(temperature=self.gumbel_temperature, logits=logits).sample())\n",
    "    \n",
    "    def train_step_func(self, n_batch):\n",
    "        \"\"\"\n",
    "        Internal function for running training for a single batch.\n",
    "        \"\"\"\n",
    "        queries_batch = tf.zeros([n_batch, self.n_columns_discrete_oh], dtype=tf.dtypes.float32)\n",
    "        \n",
    "        for i in range(self.n_critic):\n",
    "            noise = self.generate_latent(n_batch)\n",
    "            gen_data_num, gen_data_discrete = self.generator([noise, queries_batch], training=True)\n",
    "            \n",
    "            ix = np.random.randint(low=0, high=self.nrow, size=n_batch)\n",
    "            data_num_batch = self.data_num_scaled[ix]\n",
    "            data_discrete_oh_batch = self.data_discrete_oh[ix]\n",
    "            with tf.GradientTape() as discr_tape:\n",
    "                output_discr_real = self.discriminator([data_num_batch, data_discrete_oh_batch, queries_batch], training=True)\n",
    "                output_discr_fake = self.discriminator([gen_data_num, gen_data_discrete, queries_batch], training=True)\n",
    "                loss_discr = - tf.reduce_mean(output_discr_real) + tf.reduce_mean(output_discr_fake)\n",
    "                \n",
    "                epsilon = tf.random.uniform([n_batch, 1])\n",
    "                combined_data_num = epsilon * gen_data_num + (1 - epsilon) * data_num_batch\n",
    "                combined_data_discrete = epsilon * gen_data_discrete + (1 - epsilon) * data_discrete_oh_batch\n",
    "                \n",
    "                with tf.GradientTape() as discr_tape_comb:\n",
    "                    discr_tape_comb.watch(combined_data_num)\n",
    "                    discr_tape_comb.watch(combined_data_discrete)\n",
    "                    discr_tape_comb.watch(queries_batch)\n",
    "                    loss_discr_combined = self.discriminator([combined_data_num, combined_data_discrete, queries_batch], training = True)\n",
    "                combined_gradients = discr_tape_comb.gradient(loss_discr_combined, [combined_data_num, combined_data_discrete, queries_batch])\n",
    "                combined_gradients = tf.concat(combined_gradients, axis = 1)\n",
    "                \n",
    "                loss_discr_gradients = self.wgan_lambda * tf.reduce_mean((tf.norm(combined_gradients, axis = 1) - 1)**2)\n",
    "                loss_discr_combined = loss_discr + loss_discr_gradients\n",
    "            gradients_of_discriminator = discr_tape.gradient(loss_discr_combined, self.discriminator.trainable_variables)\n",
    "            self.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "        \n",
    "        queries = tf.zeros([self.data.shape[0], self.n_columns_discrete_oh], dtype=tf.dtypes.float32)\n",
    "        noise = self.generate_latent(self.data.shape[0])\n",
    "        gen_data_num, gen_data_discrete = self.generator([noise, queries], training=True)\n",
    "        output_discr_real = self.discriminator([self.data_num_scaled, self.data_discrete_oh, queries], training=True)\n",
    "        output_discr_fake = self.discriminator([gen_data_num, gen_data_discrete, queries], training=True)\n",
    "        em_distance = tf.reduce_mean(output_discr_real) - tf.reduce_mean(output_discr_fake)\n",
    "            \n",
    "        noise = self.generate_latent(n_batch)\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            gen_data_num, gen_data_discrete = self.generator([noise, queries_batch], training=True)\n",
    "            loss_gen = - tf.reduce_mean(self.discriminator([gen_data_num, gen_data_discrete, queries_batch], training=True))\n",
    "\n",
    "        gradients_of_generator = gen_tape.gradient(loss_gen, self.generator.trainable_variables)\n",
    "        self.generator_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "        \n",
    "        return em_distance, loss_gen\n",
    "    \n",
    "    def train(self, n_epochs, batch_size = None, restart_training = False, plot2D_image = False, plot2D_num_cols = [0,1],\n",
    "              plot2D_discrete_col = None, plot2D_color_opacity = 0.5, tot_save_img = 20, plot2D_save_int = None,\n",
    "              plot2D_background_func = None,\n",
    "          n_img_horiz = 5, plot2D_inv_scale = True, plot_loss = True, plot_loss_return = None,\n",
    "              loss_plot_type = \"scatter\", loss_plot_update_every = 1,\n",
    "              save_path = None, title_with_loss = False, progress_bar = False, progress_bar_desc = None,\n",
    "             n_test = None, ckpt_every = None, time_plot = False, \n",
    "             save_dir = None, filename_train_loss = \"train_loss.jpg\", filename_plot2D = \"train_plot2D.jpg\",\n",
    "              save_loss = False,\n",
    "              plot_train_loss_both = False):\n",
    "        \"\"\"\n",
    "        Function for training the data synthesizer (training the GAN architecture).\n",
    "        \"\"\"\n",
    "        if plot_loss and plot2D_image:\n",
    "            raise ValueError(\"plot_loss and plot2D_image can not both be True at the same time\")\n",
    "        \n",
    "        if (batch_size == None):\n",
    "            batch_size = self.batch_size\n",
    "        if (plot2D_save_int != None):\n",
    "            plot2D_save_epochs = np.arange(0, n_epochs, plot2D_save_int)\n",
    "            tot_save_img = len(save_epochs)\n",
    "        else:\n",
    "            plot2D_save_epochs = np.linspace(0, n_epochs, tot_save_img).astype(int)\n",
    "        \n",
    "        if plot_loss_return is None:\n",
    "            if plot_loss:\n",
    "                plot_loss_return = True\n",
    "            else:\n",
    "                plot_loss_return = False\n",
    "        \n",
    "        if (n_test == None):\n",
    "            n_test = self.nrow\n",
    "        \n",
    "        if (self.ckpt_dir != None and ckpt_every == None):\n",
    "            if (self.ckpt_every == None):\n",
    "                ckpt_every = n_epochs\n",
    "            else:\n",
    "                ckpt_every = self.ckpt_every\n",
    "                    \n",
    "        if restart_training:\n",
    "            if not self.uninitialized_opt_vars:\n",
    "                self.initialize_gan()\n",
    "            if not self.ckpt_dir is None:\n",
    "                shutil.rmtree(self.ckpt_dir)\n",
    "                os.makedirs(self.ckpt_dir, exist_ok = True)\n",
    "                self.initialize_cptk()\n",
    "        \n",
    "        self.uninitialized_opt_vars = False\n",
    "\n",
    "        batch_per_epoch = int(self.nrow / batch_size)\n",
    "\n",
    "        n_img_vert = ceil(tot_save_img / n_img_horiz)\n",
    "        \n",
    "        fig_loss, ax_loss = plt.subplots(1,1, figsize=(12, 6))\n",
    "        ax_loss.hlines(0, self.start_epoch, self.start_epoch + n_epochs, color = \"black\", linestyle = \"dashed\")\n",
    "        \n",
    "        if plot2D_image:\n",
    "            fig_plot2D = plt.figure(figsize=(16, 16/n_img_horiz*n_img_vert))\n",
    "            #fig_plot2D.suptitle('Visalization of counterfactual generator training for %d epochs' % n_epochs)\n",
    "            noise_test = self.generate_latent(n_test)\n",
    "            queries_test = tf.zeros([n_test, self.n_columns_discrete_oh])\n",
    "            \n",
    "        if plot_loss:\n",
    "            hdisplay = display(\"\", display_id=True)\n",
    "            \n",
    "        if time_plot:\n",
    "            time_epoch_vec = np.zeros(n_epochs+1)\n",
    "    \n",
    "        \n",
    "        gen_loss_vec = np.zeros(n_epochs)\n",
    "        em_distance_vec = np.zeros(n_epochs)\n",
    "        epochs = np.arange(self.start_epoch+1, self.start_epoch + n_epochs + 1)\n",
    "            \n",
    "\n",
    "        img_count = 1\n",
    "        with tqdm(total = n_epochs, leave = False, disable = not progress_bar, desc = progress_bar_desc) as pbar:\n",
    "            # manually enumerate epochs\n",
    "            for epoch in range(0, n_epochs+1):\n",
    "                if (time_plot):\n",
    "                    time_before_epoch = time.perf_counter()\n",
    "                if (epoch > 0):\n",
    "                    em_distance = g_loss = 0\n",
    "                    for batch in range(batch_per_epoch):\n",
    "                        em_distance_batch, gen_loss_batch = self.train_step(batch_size)\n",
    "                        g_loss += gen_loss_batch\n",
    "                        em_distance += em_distance_batch\n",
    "                    g_loss /= batch_per_epoch\n",
    "                    em_distance /= batch_per_epoch\n",
    "\n",
    "                    gen_loss_vec[epoch - 1] = g_loss\n",
    "                    em_distance_vec[epoch - 1] = em_distance\n",
    "\n",
    "                    if plot_loss:\n",
    "                        if (epoch % loss_plot_update_every == 0 or epoch in [1, n_epochs]):\n",
    "                            if (epoch > 1):\n",
    "                                scatter_discr.remove()\n",
    "                                if plot_train_loss_both:\n",
    "                                    scatter_gen.remove()\n",
    "\n",
    "                            scatter_discr = ax_loss.scatter(epochs[:(epoch-1)], em_distance_vec[:(epoch-1)],\n",
    "                                                        color = \"red\", label = \"EM distance\")\n",
    "                            if plot_train_loss_both:\n",
    "                                scatter_gen = ax_loss.scatter(epochs[:(epoch-1)], gen_loss_vec[:(epoch-1)],\n",
    "                                                          color = \"blue\", label = \"Generator loss\")\n",
    "                            if (epoch == 1):\n",
    "                                ax_loss.legend()\n",
    "                            hdisplay.update(fig_loss)\n",
    "\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                if self.ckpt_dir != None:\n",
    "                    if epoch % ckpt_every == 0 or epoch == n_epochs:\n",
    "                        if os.path.exists(os.path.join(self.ckpt_dir, \"checkpoint\")):\n",
    "                            os.remove(os.path.join(self.ckpt_dir, \"checkpoint\"))\n",
    "                        self.ckpt_manager.save(self.ckpt.epoch.numpy())\n",
    "                    self.ckpt.epoch.assign_add(1)\n",
    "                if plot2D_image:\n",
    "                    if epoch in plot2D_save_epochs:\n",
    "                        ax_plot2D = fig_plot2D.add_subplot(n_img_vert, n_img_horiz, img_count)\n",
    "                        gen_data_num, gen_data_discrete = self.generator([noise_test, queries_test])\n",
    "                        \n",
    "                        if not plot2D_background_func is None:\n",
    "                            plot2D_background_func(ax_plot2D)\n",
    "                        \n",
    "                        if (plot2D_inv_scale):\n",
    "                            gen_data_num = self.scaler_num.inverse_transform(gen_data_num)\n",
    "                            gen_data_discrete = self.oh_encoder.inverse_transform(gen_data_discrete)\n",
    "                            gen_data_discrete = pd.DataFrame(gen_data_discrete, columns = self.columns_discrete)\n",
    "                            if plot2D_discrete_col is None:\n",
    "                                color = None\n",
    "                            else:\n",
    "                                labels_unique = np.unique(gen_data_discrete[plot2D_discrete_col])\n",
    "                                colors_unique = map_str_to_color(labels_unique)\n",
    "                                color_dict = {label : color for label, color in zip(labels_unique, colors_unique)}\n",
    "                                color = gen_data_discrete[plot2D_discrete_col].map(color_dict)\n",
    "                                \n",
    "                            if img_count == 1:\n",
    "                                markers = [plt.Line2D([0,0],[0,0], color=color, marker='o',\n",
    "                                                      linestyle='', alpha=0.5) for color in color_dict.values()]\n",
    "                                plt.legend(markers, color_dict.keys(), numpoints=1)\n",
    "                        ax_plot2D.scatter(gen_data_num[:,plot2D_num_cols[0]], gen_data_num[:,plot2D_num_cols[1]],\n",
    "                                         c =color, alpha = plot2D_color_opacity)\n",
    "                        ax_plot2D.set_title(\"Epoch %d/%d\" % (epoch, n_epochs))\n",
    "                        plt.tight_layout()\n",
    "                        display(fig_plot2D)\n",
    "                        clear_output(wait = True)\n",
    "                        img_count +=1\n",
    "\n",
    "\n",
    "                if (time_plot):\n",
    "                    time_epoch_vec[epoch] = time.perf_counter() - time_before_epoch\n",
    "                \n",
    "        plt.close(fig_loss)\n",
    "        fig_loss, ax_loss = plt.subplots(1,1, figsize=(12, 6))\n",
    "        ax_loss.hlines(0, self.start_epoch, self.start_epoch + n_epochs, color = \"black\", linestyle = \"dashed\")\n",
    "        if (loss_plot_type == \"scatter\"):\n",
    "            ax_loss.scatter(epochs, em_distance_vec, color = \"red\", label = \"EM distance\")\n",
    "            if plot_train_loss_both:\n",
    "                ax_loss.scatter(epochs, gen_loss_vec, color = \"blue\", label = \"Generator loss\")\n",
    "        elif loss_plot_type == \"line\":\n",
    "            ax_loss.plot(epochs, em_distance_vec, color = \"red\", label = \"EM distance\")\n",
    "            if plot_train_loss_both:\n",
    "                ax_loss.plot(epochs, gen_loss_vec, color = \"blue\", label = \"Generator loss\")\n",
    "        else:\n",
    "            raise Exception(\"Unknown loss_plot_type. Only scatter and line implemented.\")\n",
    "        ax_loss.legend()\n",
    "        \n",
    "        plt.close(fig_loss)\n",
    "        \n",
    "        if not (plot_loss_return or time_plot or time_plot):\n",
    "            return None\n",
    "        \n",
    "        return_figures = ()\n",
    "        \n",
    "        if plot_loss_return:\n",
    "            return_figures += (fig_loss,)\n",
    "        \n",
    "        if (plot2D_image):\n",
    "            plt.close(fig_plot2D)\n",
    "            return_figures += (fig_plot2D,)\n",
    "        \n",
    "        if (time_plot):\n",
    "            epochs = np.arange(self.start_epoch, self.start_epoch + n_epochs + 1)\n",
    "            fig_time_plot = plt.figure()\n",
    "            plt.plot(epochs, time_epoch_vec)\n",
    "            plt.title(\"Time for each epoch\")\n",
    "            plt.close(fig_time_plot)\n",
    "            return_figures += (fig_time_plot,)\n",
    "        \n",
    "        if not save_dir is None:\n",
    "            if plot2D_image and (not filename_plot2D is None):\n",
    "                save_path = os.path.join(save_dir, filename_plot2D)\n",
    "                os.makedirs(save_dir, exist_ok = True)\n",
    "                fig_plot2D.savefig(save_path)\n",
    "            \n",
    "            if (plot_loss or save_loss) and not filename_train_loss is None:\n",
    "                save_path = os.path.join(save_dir, filename_train_loss)\n",
    "                os.makedirs(save_dir, exist_ok = True)\n",
    "                fig_loss.savefig(save_path)\n",
    "        \n",
    "        return(return_figures)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def restore_checkpoint(self, epoch = \"latest\"):\n",
    "        \"\"\"\n",
    "        Function for restoring saved model checkpoint\n",
    "        \"\"\"\n",
    "        if(epoch == \"latest\"):\n",
    "            ckpt_path = self.ckpt_manager.latest_checkpoint\n",
    "        else:\n",
    "            ckpt_path = self.ckpt_prefix + \"-\" + str(epoch)\n",
    "        if self.uninitialized_opt_vars:\n",
    "            self.ckpt.restore(ckpt_path).expect_partial()\n",
    "        else:\n",
    "            self.ckpt.restore(ckpt_path).assert_consumed()\n",
    "        start_epoch = self.ckpt.epoch.numpy()\n",
    "        \n",
    "    def use_critic_on_data(self, data):\n",
    "        \"\"\"\n",
    "        Internal function for preprocessing data and then fetching it to the critic. Mostly used for debugging purposes.\n",
    "        \"\"\"\n",
    "        num_data = data[self.columns_num]\n",
    "        discrete_data = data[self.columns_discrete]\n",
    "        num_data_scaled = self.scaler_num.transform(num_data)\n",
    "        discrete_data_oh = self.oh_encoder.transform(discrete_data)\n",
    "        queries_batch = tf.zeros([data.shape[0], self.n_columns_discrete_oh], dtype=tf.dtypes.float32)\n",
    "        return(self.discriminator.predict([num_data_scaled, discrete_data_oh, queries_batch]))\n",
    "        \n",
    "        \n",
    "    def generate_counterfactuals(self, n_to_keep, pred_func, x_obs, wanted_range = None, n_to_generate = None,\n",
    "                                add_plausibility_objective = False, epsilon_num_percent = 0.005,\n",
    "                                return_objectives = True):\n",
    "        \"\"\"\n",
    "        Function for generating counterfactuals\n",
    "        \"\"\"\n",
    "        if n_to_generate is None:\n",
    "            n_to_generate = n_to_keep * 1000\n",
    "        if wanted_range is None:\n",
    "            wanted_label = 1 - np.round(pred_func(x_obs))\n",
    "            wanted_range = np.sort([0.5, wanted_label])\n",
    "        \n",
    "        gen_data = self.generate_data(n_to_generate)\n",
    "        pred_gen_data = pred_func(gen_data)\n",
    "        gen_data = gen_data.loc[(pred_gen_data >= wanted_range[0]) & (pred_gen_data <= wanted_range[1])].reset_index(drop=True)\n",
    "        if gen_data.shape[0] == 0:\n",
    "            raise RuntimeError(\"None of the generated observations had a prediction value in the wanted range\")\n",
    "        \n",
    "        n_objectives = 2 \n",
    "        objective_names = [\"Gower distance\", \"Number changed\"]\n",
    "        if add_plausibility_objective:\n",
    "            n_objectives += 1\n",
    "            objective_names += [\"Plausibility\"]\n",
    "        objectives = np.zeros([gen_data.shape[0], n_objectives])\n",
    "        range_num_values_dict = {}\n",
    "        for i, col_num in enumerate(self.columns_num):\n",
    "            range_num_values_dict[col_num] = np.max(self.data[col_num]) - np.min(self.data[col_num])\n",
    "        \n",
    "        for i, col_num in enumerate(self.columns_num):\n",
    "            objectives[:, 0] += np.abs(gen_data[col_num].to_numpy() - x_obs[col_num].to_numpy()) / range_num_values_dict[col_num]\n",
    "            objectives[:, 1] += np.where(np.isclose(gen_data[col_num], x_obs[col_num],\n",
    "                                                   atol= epsilon_num_percent * range_num_values_dict[col_num]),\n",
    "                                         0, 1)\n",
    "            \n",
    "        for i, col_discrete in enumerate(self.columns_discrete):\n",
    "            binary_is_cat_changed = np.where(gen_data[col_discrete].to_numpy() == x_obs[col_discrete].to_numpy(), 0, 1)\n",
    "            objectives[:, 0] += binary_is_cat_changed\n",
    "            objectives[:, 1] += binary_is_cat_changed\n",
    "        \n",
    "        objectives[:,0] /= self.n_columns\n",
    "        \n",
    "        if add_plausibility_objective:\n",
    "            raise ValueError(\"Not yet implemented\")\n",
    "        \n",
    "        rank_list = fast_non_dominated_sort(objectives, minimize = True)\n",
    "        n_each_rank = [len(rank_group) for rank_group in rank_list]\n",
    "        cumsum_each_rank = np.cumsum(n_each_rank)\n",
    "        if cumsum_each_rank[-1] < n_to_keep:\n",
    "            raise RuntimeError(\"Not enough valid counterfactuals were generated, increase the parameter n_to_generate to decrease the chance of this happening.\")\n",
    "        last_needed_rank = np.where(np.greater_equal(cumsum_each_rank, n_to_keep))[0][0]\n",
    "        rank_list = rank_list[:last_needed_rank+1]\n",
    "        \n",
    "        ranking = [index for rank_group in rank_list for index in rank_group]\n",
    "        \n",
    "        gen_data = gen_data.iloc[ranking].head(n_to_keep).reset_index(drop=True)\n",
    "        if return_objectives:\n",
    "            df_objectives = pd.DataFrame(objectives[ranking[:n_to_keep],], columns=objective_names)\n",
    "            gen_data = gen_data.join(df_objectives)\n",
    "        return gen_data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
