{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a15f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_hist_real_vs_generated(model, other_gen_dataset = None, n_img_horiz = 4, n_bins = 20, figsize = None,\n",
    "                                   discrete_xtick_rotation = 45,\n",
    "                                   title = None, seed = None, save_path = None, save_dir = None):\n",
    "    if (seed != None):\n",
    "        tf.random.set_seed(seed)\n",
    "    if other_gen_dataset is None:\n",
    "        gen_data = model.generate_data()\n",
    "    else:\n",
    "        gen_data = other_gen_dataset\n",
    "    n_img_vert = (model.n_columns-1) // n_img_horiz + 1\n",
    "    if(figsize == None):\n",
    "        figsize = (15, 3*n_img_vert)\n",
    "    fig, ax = plt.subplots(n_img_vert, n_img_horiz, figsize = figsize)\n",
    "    fig.suptitle(title)\n",
    "    img_counter_horiz = 0\n",
    "    img_counter_vert = 0\n",
    "    img_counter = 0\n",
    "    for num_col in model.columns_num:\n",
    "        min_val = min(min(gen_data[num_col]), min(model.data[num_col]))\n",
    "        max_val = max(max(gen_data[num_col]),  max(model.data[num_col]))\n",
    "        bins = np.linspace(min_val, max_val, n_bins + 1, dtype = np.float)\n",
    "        ax[img_counter_vert, img_counter_horiz].hist(model.data[num_col], density = True, alpha = 0.5,\n",
    "                                                     bins = bins, label = \"Real\")\n",
    "        ax[img_counter_vert, img_counter_horiz].hist(gen_data[num_col], density = True, alpha = 0.5,\n",
    "                                                     bins = bins, label = \"Gen\")\n",
    "        ax[img_counter_vert, img_counter_horiz].set_title(num_col)\n",
    "        ax[img_counter_vert, img_counter_horiz].legend()\n",
    "        img_counter_horiz += 1\n",
    "        img_counter += 1\n",
    "        if (img_counter_horiz == n_img_horiz):\n",
    "            img_counter_horiz = 0\n",
    "            img_counter_vert += 1\n",
    "    for discrete_col in model.columns_discrete:\n",
    "        unique_values = np.unique(model.data[discrete_col])\n",
    "        map_discr_to_int = {s : i for i,s in enumerate(unique_values)}\n",
    "        bins = np.arange(0, unique_values.size, 0.5) - 0.25\n",
    "        x_ticks = np.arange(0, unique_values.size)\n",
    "        ax[img_counter_vert, img_counter_horiz].hist(model.data[discrete_col].map(map_discr_to_int),\n",
    "                                                     bins = bins, density = True, alpha = 0.5, label = \"Real\")\n",
    "        ax[img_counter_vert, img_counter_horiz].hist(gen_data[discrete_col].map(map_discr_to_int),\n",
    "                                                     bins = bins, density = True, alpha = 0.5, label = \"Gen\")\n",
    "        ax[img_counter_vert, img_counter_horiz].set_title(discrete_col)\n",
    "        ax[img_counter_vert, img_counter_horiz].set_xticks(x_ticks)\n",
    "        ax[img_counter_vert, img_counter_horiz].set_xticklabels(unique_values)\n",
    "        ax[img_counter_vert, img_counter_horiz].tick_params(axis='x', labelrotation = discrete_xtick_rotation)\n",
    "        ax[img_counter_vert, img_counter_horiz].legend()\n",
    "        img_counter_horiz += 1\n",
    "        img_counter += 1\n",
    "        if (img_counter_horiz == n_img_horiz):\n",
    "            img_counter_horiz = 0\n",
    "            img_counter_vert += 1\n",
    "    for i in range(n_img_horiz*n_img_vert - img_counter):\n",
    "        ax[img_counter_vert, img_counter_horiz].axis(\"off\")\n",
    "        img_counter_horiz += 1\n",
    "        if (img_counter_horiz == n_img_horiz):\n",
    "            img_counter_horiz = 0\n",
    "            img_counter_vert += 1\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if not (save_path is None):\n",
    "        if not save_dir is None:\n",
    "            os.makedirs(save_dir, exist_ok = True)\n",
    "            save_path = os.path.join(save_dir, save_path)\n",
    "        plt.savefig(save_path)\n",
    "    \n",
    "    plt.close(fig)\n",
    "    \n",
    "    return(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a06961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_str_to_color(vec, color_map = \"viridis\"):\n",
    "    cmap = matplotlib.cm.get_cmap(color_map)\n",
    "    vec_set = set(vec)\n",
    "    colors = cmap(np.linspace(0, 1, len(vec_set)))\n",
    "    clr_map = {string : colors[i] for i, string in enumerate(vec_set)}\n",
    "    return([clr_map[s] for s in vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89893f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_evolution_hist_real_vs_generated(tg, epochs = None, name = \"compare_hist_real_vs_generated\", fps = 1,\n",
    "                                             mult_time_first_image = 1, mult_time_last_image = 1, **kwargs):\n",
    "    name += \".gif\"\n",
    "    if epochs is None:\n",
    "        ckpts = tg.ckpt_manager.checkpoints\n",
    "        epochs = [int(ckpt.replace(tg.ckpt_prefix + \"-\", \"\")) for ckpt in ckpts]\n",
    "    \n",
    "    dir_path = \".//temp_gif_compare_hist_real_vs_generated//\"\n",
    "    os.makedirs(dir_path, exist_ok = True)\n",
    "    filenames = []\n",
    "    for i in tqdm(range(len(epochs))):\n",
    "        # plot the line chart\n",
    "        tg.restore_checkpoint(epoch = epochs[i])\n",
    "        \n",
    "        # create file name and append it to a list\n",
    "        filename = f'{dir_path}{epochs[i]}.jpg'\n",
    "        filenames.append(filename)\n",
    "        \n",
    "        # Create figure for current epoch\n",
    "        fig = compare_hist_real_vs_generated(tg, title = \"Epoch %d\" % (epochs[i]), save_path = filename)\n",
    "\n",
    "    # build gif\n",
    "    last_image_i = len(epochs)\n",
    "    with imageio.get_writer(name, mode='I', fps = 1) as writer:\n",
    "        for i, filename in enumerate(filenames):\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)\n",
    "            if (i == 0):\n",
    "                for j in range(1, mult_time_first_image):\n",
    "                    writer.append_data(image)\n",
    "            if (i == last_image_i):\n",
    "                for j in range(1, mult_time_last_image):\n",
    "                    writer.append_data(image)\n",
    "\n",
    "    # Remove files\n",
    "    shutil.rmtree(dir_path)\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aec6a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nmi_matrix(tgan = None, dataset = None, bins = None, n_q_bins = 40\n",
    "                       , generated_data = True, retbins = False, average_method = \"arithmetic\",\n",
    "                      n_samples = None):\n",
    "    if n_samples is None:\n",
    "        n_samples = tgan.data.shape[0]\n",
    "    \n",
    "    if dataset is None:\n",
    "        if (generated_data):\n",
    "            data_binned = tgan.generate_data(n_samples)\n",
    "        else:\n",
    "            data_binned = tgan.data.copy()\n",
    "    else:\n",
    "        data_binned = dataset.copy()\n",
    "    if (retbins):\n",
    "        bins_curr = {}\n",
    "    for col_num in tgan.columns_num:\n",
    "        if bins == None:\n",
    "            cut_series, cut_bins = pd.qcut(data_binned[col_num] , q = n_q_bins, retbins=True, duplicates = \"drop\")\n",
    "        else:\n",
    "            cut_series, cut_bins = pd.cut(data_binned[col_num], bins = bins[col_num], retbins = True,\n",
    "                                         include_lowest = True)\n",
    "        data_binned[col_num] = cut_series\n",
    "        if (retbins):\n",
    "            bins_curr[col_num] = cut_bins\n",
    "            \n",
    "    if average_method == \"arithmetic\":\n",
    "        average_func = lambda x,y : np.mean([x,y])\n",
    "    elif average_method == \"max\":\n",
    "        average_func = lambda x, y : np.max([x,y])\n",
    "    elif average_method == \"min\":\n",
    "        average_func = lambda x, y : np.min([x,y])\n",
    "    elif average_method == \"geometric\":\n",
    "        average_func = lambda x, y : np.sqrt(x * y)\n",
    "    else:\n",
    "        raise ValueError(\"Average_method given as input is not implemented\")\n",
    "    \n",
    "    probs_dict = {} \n",
    "    entropy = np.zeros(tg.n_columns)\n",
    "    for i,col in enumerate(tgan.columns):\n",
    "        col_category_fractions = data_binned[col].value_counts(normalize = True)\n",
    "        probs_dict[col] = col_category_fractions.to_dict()\n",
    "        entropy[i] = np.sum(- col_category_fractions * np.log(col_category_fractions))\n",
    "    \n",
    "    nmi_matrix = np.zeros([tg.n_columns, tg.n_columns])\n",
    "    for i,col1 in enumerate(tg.columns):\n",
    "        for j,col2 in enumerate(tg.columns):\n",
    "            if j < i:\n",
    "                continue\n",
    "            elif i == j:\n",
    "                nmi_matrix[i,j] = 1\n",
    "                continue\n",
    "            df_curr_cols = data_binned[[col1,col2]].copy()\n",
    "            df_curr_cols_fraction = df_curr_cols.groupby([col1,col2]).size().reset_index().rename(columns={0:\"Prob.both\"})\n",
    "            df_curr_cols_fraction[\"Prob.both\"] /= data_binned.shape[0]\n",
    "            df_curr_cols_fraction[\"Prob.col1\"] = df_curr_cols_fraction[col1].map(probs_dict[col1]).astype(float)\n",
    "            df_curr_cols_fraction[\"Prob.col2\"] = df_curr_cols_fraction[col2].map(probs_dict[col2]).astype(float)\n",
    "            df_curr_cols_fraction[\"NMI\"] = df_curr_cols_fraction[\"Prob.both\"] * np.log(df_curr_cols_fraction[\"Prob.both\"]/\n",
    "                                                                                       (df_curr_cols_fraction[\"Prob.col1\"]*df_curr_cols_fraction[\"Prob.col2\"]))\n",
    "            nmi_matrix[i,j] = nmi_matrix[j,i] = np.sum(df_curr_cols_fraction[\"NMI\"]) / average_func(entropy[i], entropy[j])\n",
    "    \n",
    "    if retbins:\n",
    "        return nmi_matrix, bins_curr\n",
    "    else:\n",
    "        return nmi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43b260e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_nmi_matrices(tgans, extra_datasets = None, include_true_data = True, n_q_bins = 40, ncol = None, nrow = None,\n",
    "                        average_method = \"arithmetic\", subplot_title_true_dataset = \"True dataset\",\n",
    "                         subplot_titles_tgans = None, subplot_titles_extra_datasets = None, figsize = [14,5],\n",
    "                         compute_diff_nmi_matrices = False, save_dir = None, save_name = None, title = None,\n",
    "                        data_test = None):\n",
    "    if (not save_dir is None) and save_name is None:\n",
    "        if compute_diff_nmi_matrices:\n",
    "            save_name = \"nmi_diff_matrices\"\n",
    "        else:\n",
    "            save_name = \"nmi_matrices\"\n",
    "        \n",
    "    if subplot_titles_tgans == None:\n",
    "        subplot_titles_tgans = [None] * len(tgans)\n",
    "    else:\n",
    "        if (len(subplot_titles_tgans) != len(tgans)):\n",
    "            raise ValueError(\"Number of tgan subplot titles must be equal to number of tgans\")\n",
    "    \n",
    "    if include_true_data:\n",
    "        subplot_titles_tgans = [subplot_title_true_dataset] + subplot_titles_tgans\n",
    "        \n",
    "    if not extra_datasets is None:\n",
    "        datasets = [None] * len(tgans) + extra_datasets\n",
    "        tgans = tgans + [tgans[0]] * len(extra_datasets)\n",
    "        if (subplot_titles_extra_datasets == None):\n",
    "            subplot_titles_extra_datasets = [None] * len(extra_datasets)\n",
    "        else:\n",
    "            if (len(subplot_titles_extra_datasets) != len(extra_datasets)):\n",
    "                raise ValueError(\"Number of subplot titles for the extra datasets must be equal to the number of extra datasets\")\n",
    "        subplot_titles = subplot_titles_tgans + subplot_titles_extra_datasets\n",
    "    else:\n",
    "        subplot_titles = subplot_titles_tgans\n",
    "        datasets = [None] * len(tgans)\n",
    "    \n",
    "    n_subplots = len(tgans) + (1 if include_true_data else 0)\n",
    "    \n",
    "    def map_fig_n_to_indices(curr_fig, ncol, nrow):\n",
    "        if ncol == 1:\n",
    "            return curr_fig\n",
    "        elif nrow == 1:\n",
    "            return curr_fig\n",
    "        else:\n",
    "            curr_fig_col = floor(curr_fig // ncol)\n",
    "            curr_fig_row = curr_fig - curr_fig_col * ncol\n",
    "            return curr_fig_col, curr_fig_row\n",
    "        \n",
    "    if ncol == None and nrow == None:\n",
    "        nrow = 1\n",
    "        ncol = n_subplots\n",
    "    elif ncol == None:\n",
    "        ncol = ceil(n_subplots / nrow)\n",
    "    elif nrow == None:\n",
    "        nrow = ceil(n_subplots / ncol)\n",
    "    else:\n",
    "        if (nrow * ncol < n_subplots):\n",
    "            raise ValueError(\"ncol times nrow must be larger than number of subfigures to plot\")\n",
    "        \n",
    "    if data_test is None:\n",
    "        n_samples = tgans[0].data.shape[0]\n",
    "    else:\n",
    "        n_samples = data_test.shape[0]\n",
    "    \n",
    "    fig, axes = plt.subplots(nrow, ncol, figsize = figsize, sharey = True, sharex=True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.title(title)\n",
    "    \n",
    "    curr_fig = 0\n",
    "    nmi_matrix_truth_train, bins = compute_nmi_matrix(tgans[curr_fig], bins = None, n_q_bins = n_q_bins, generated_data = False,\n",
    "                                          retbins = True, average_method = average_method)\n",
    "    \n",
    "    if compute_diff_nmi_matrices:\n",
    "#         colors_blue = plt.cm.Blues(np.linspace(0., 1, 128))\n",
    "#         colors_red = np.flip(plt.cm.Reds(np.linspace(0, 1, 128)))\n",
    "#         colors = np.vstack((colors_red, colors_blue))\n",
    "#         cmap_diff_nmi = mcolors.LinearSegmentedColormap.from_list('my_blue_red_colormap', colors)\n",
    "        cmap_diff_nmi = plt.cm.bwr_r\n",
    "#         cmap_diff_nmi = sns.diverging_palette(250, 10, sep = 1, n=200, s=100, as_cmap=True)\n",
    "    \n",
    "    if (include_true_data):\n",
    "        if data_test is None:\n",
    "            nmi_matrix_truth = nmi_matrix_truth_train\n",
    "        else:\n",
    "            nmi_matrix_truth = compute_nmi_matrix(tgans[curr_fig], dataset = data_test, bins = None, n_q_bins = n_q_bins,\n",
    "                                                  generated_data = True, retbins = False, average_method = average_method,\n",
    "                                                  n_samples = n_samples)\n",
    "            \n",
    "        axes_ind = map_fig_n_to_indices(0, ncol, nrow)\n",
    "        if compute_diff_nmi_matrices:\n",
    "            axes[axes_ind].imshow(nmi_matrix_truth - nmi_matrix_truth_train, cmap = cmap_diff_nmi, vmin = -1, vmax = 1)\n",
    "        else:\n",
    "            axes[axes_ind].imshow(nmi_matrix_truth, cmap = plt.cm.Blues)\n",
    "        axes[axes_ind].set_xticks([])\n",
    "        axes[axes_ind].set_yticks([])\n",
    "        if subplot_titles != None:\n",
    "            axes[axes_ind].set_title(subplot_titles[curr_fig])\n",
    "    \n",
    "    curr_tgan = 0\n",
    "    for curr_fig in range(1 if include_true_data else 0, nrow * ncol):\n",
    "        if (curr_fig < n_subplots):\n",
    "            nmi_matrix = compute_nmi_matrix(tgans[curr_tgan], dataset = datasets[curr_tgan], n_q_bins = n_q_bins,\n",
    "                                            generated_data = True, retbins = False, average_method = average_method,\n",
    "                                           n_samples = n_samples)\n",
    "            axes_ind = map_fig_n_to_indices(curr_fig, ncol, nrow)\n",
    "            if compute_diff_nmi_matrices:\n",
    "                nmi_matrix -= nmi_matrix_truth_train\n",
    "                im = axes[axes_ind].imshow(nmi_matrix, cmap = cmap_diff_nmi, vmin = -1, vmax = 1)\n",
    "            else:\n",
    "                im = axes[axes_ind].imshow(nmi_matrix, cmap = plt.cm.Blues)\n",
    "            #axes[axes_ind].set_xticks([])\n",
    "            #axes[axes_ind].set_yticks([])\n",
    "            xticks = axes[axes_ind].set_xticks(np.arange(0, nmi_matrix_truth.shape[0]))\n",
    "            yticks = axes[axes_ind].set_yticks(np.arange(0, nmi_matrix_truth.shape[0]))\n",
    "            xticklabels = axes[axes_ind].set_xticklabels(tgans[curr_tgan].columns, rotation = 90)\n",
    "            yticklabels = axes[axes_ind].set_yticklabels(tgans[curr_tgan].columns)\n",
    "            if subplot_titles != None:\n",
    "                axes[axes_ind].set_title(subplot_titles[curr_fig])\n",
    "            curr_tgan += 1\n",
    "        else:\n",
    "            axes[map_fig_n_to_indices(curr_fig, ncol, nrow)].axis(\"off\")\n",
    "\n",
    "    fig.colorbar(im, ax=axes.ravel().tolist())\n",
    "    if not save_dir is None:\n",
    "        plt.savefig(os.path.join(save_dir, save_name))\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc7be4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self._start_time = {}\n",
    "        self.elapsed_time = {}\n",
    "\n",
    "    def start(self, name):\n",
    "        \"\"\"Start a new timer\"\"\"\n",
    "\n",
    "        self._start_time[name] = time.perf_counter()\n",
    "\n",
    "    def stop(self, name):\n",
    "        \"\"\"Stop the timer, and report the elapsed time\"\"\"\n",
    "        curr_elapsed_time = time.perf_counter() - self._start_time[name]\n",
    "        self.elapsed_time[name] = curr_elapsed_time\n",
    "        self._start_time[name] = None\n",
    "        print(f\"Elapsed time for {name}: {curr_elapsed_time:0.3f} seconds\")\n",
    "        \n",
    "    def save(self, path, save_dir = None):\n",
    "        if not save_dir is None:\n",
    "            path = os.path.join(save_dir, path)\n",
    "            os.makedirs(save_dir, exist_ok = True)\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "    \n",
    "    def load(self, path, save_dir = None):\n",
    "        if not save_dir is None:\n",
    "            path = os.path.join(save_dir, path)\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "def load_timer(path, save_dir = None):\n",
    "    if not save_dir is None:\n",
    "        path = os.path.join(save_dir, path)\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ee13760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate_xgboost(data_train, data_test, categories = \"auto\", retcats = False, response_col = \"income\"):\n",
    "    X_train = data_train.iloc[:,data_train.columns != response_col]\n",
    "    Y_train = data_train[response_col]\n",
    "    X_test = data_test.iloc[:,data_train.columns != response_col]\n",
    "    Y_test = data_test[response_col]\n",
    "    \n",
    "    columns_discrete_bool = [b in [\"object\", \"category\"] for b in X_train.dtypes]\n",
    "    discrete_columns_of_X = X_train.columns[columns_discrete_bool]\n",
    "    numeric_columns_of_X = X_train.columns[np.logical_not(columns_discrete_bool)]\n",
    "    \n",
    "    \n",
    "    X_train[discrete_columns_of_X].astype(\"category\")\n",
    "    X_test[discrete_columns_of_X].astype(\"category\")\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    Y_train = label_encoder.fit_transform(Y_train)\n",
    "    Y_test = label_encoder.transform(Y_test)\n",
    "    \n",
    "    #categories = OneHotEncoder().fit(X_train[discrete_columns_of_X]).categories_\n",
    "    oh_encoder = OneHotEncoder(categories = categories, sparse = False)\n",
    "    X_train_discr = oh_encoder.fit_transform(X_train[discrete_columns_of_X])\n",
    "    X_train = np.concatenate((X_train[numeric_columns_of_X].to_numpy(),\n",
    "                              oh_encoder.fit_transform(X_train[discrete_columns_of_X])),\n",
    "                             axis = 1)\n",
    "    X_test = np.concatenate((X_test[numeric_columns_of_X].to_numpy(),\n",
    "                              oh_encoder.transform(X_test[discrete_columns_of_X])),\n",
    "                             axis = 1)\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        tree_method=\"gpu_hist\", enable_categorical=False, use_label_encoder=False, eval_metric = \"logloss\"\n",
    "    )\n",
    "    # X is the dataframe we created in previous snippet\n",
    "    clf.fit(X_train, Y_train)\n",
    "    # Must use JSON for serialization, otherwise the information is lost\n",
    "    clf.save_model(\"categorical-model.json\")\n",
    "    clf.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(Y_test, clf.predict(X_test))\n",
    "    auc = roc_auc_score(Y_test, clf.predict_proba(X_test)[:,1])\n",
    "    \n",
    "    if retcats:\n",
    "        return accuracy, auc, oh_encoder.categories_\n",
    "    else:\n",
    "        return accuracy, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ddc622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tabGAN_through_prediction(data_train, data_test, dataset_dir, subfolders, n_synthetic_datasets,\n",
    "                                      name_true_train_dataset = \"Train dataset\",\n",
    "                                       eval_sd_true_dataset = False, print_all_accuracy = False):\n",
    "    progress_bar_total = len(subfolders)*n_synthetic_datasets + (n_synthetic_datasets if eval_sd_true_dataset else 1)\n",
    "    with tqdm(total=progress_bar_total) as pbar:\n",
    "        models = [name_true_train_dataset] + subfolders\n",
    "        result = pd.DataFrame({\"Dataset\" : models, \"Test Accuracy\" : None, \"Test AUC\" : None,\n",
    "                              \"SD Accuracy\" : None, \"SD AUC\" : None})\n",
    "        if eval_sd_true_dataset:\n",
    "            accuracy_vec = np.zeros(n_synthetic_datasets)\n",
    "            auc_vec = np.zeros(n_synthetic_datasets)\n",
    "            for j in range(n_synthetic_datasets):\n",
    "                accuracy_curr, auc_curr, categories = fit_and_evaluate_xgboost(data_train, data_test, retcats=True)\n",
    "                accuracy_vec[j] = accuracy_curr\n",
    "                auc_vec[j] = auc_curr\n",
    "                pbar.update(1)\n",
    "            accuracy = np.mean(accuracy_vec)\n",
    "            auc = np.mean(auc_vec)\n",
    "            accuracy_std = np.std(accuracy_vec)\n",
    "            auc_std = np.std(auc_vec)\n",
    "        else:\n",
    "            accuracy, auc, categories = fit_and_evaluate_xgboost(data_train, data_test, retcats = True)\n",
    "            accuracy_std, auc_std = 0, 0\n",
    "            pbar.update(1)\n",
    "        result.iloc[0, 1:] = [accuracy, auc, accuracy_std, auc_std]\n",
    "        \n",
    "\n",
    "        for i, subfolder in enumerate(subfolders, start=1):\n",
    "            accuracy_vec = np.zeros(n_synthetic_datasets)\n",
    "            auc_vec = np.zeros(n_synthetic_datasets)\n",
    "            curr_dataset_dir = os.path.join(dataset_dir, subfolder)\n",
    "            for j in range(n_synthetic_datasets):\n",
    "                path = os.path.join(curr_dataset_dir, f\"gen{j}.csv\")\n",
    "                fake_train = pd.read_csv(path, index_col = 0)\n",
    "                eval_result = fit_and_evaluate_xgboost(fake_train, data_test, categories = categories)\n",
    "                accuracy_vec[j] = eval_result[0]\n",
    "                auc_vec[j] = eval_result[1]\n",
    "                pbar.update(1)\n",
    "            if print_all_accuracy:\n",
    "                print(subfolder, accuracy_vec)\n",
    "            accuracy = np.mean(accuracy_vec)\n",
    "            auc = np.mean(auc_vec)\n",
    "            accuracy_std = np.std(accuracy_vec)\n",
    "            auc_std = np.std(auc_vec)\n",
    "            result.iloc[i,1:] = [accuracy, auc, accuracy_std, auc_std]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c4c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_non_dominated_sort(func_value_matrix, minimize = False):\n",
    "    if minimize:\n",
    "        evaluator = np.less_equal\n",
    "    else:\n",
    "        evaluator = np.greater_equal\n",
    "    nrow = func_value_matrix.shape[0]\n",
    "    ncol = func_value_matrix.shape[1]\n",
    "    S = [[] for i in range(nrow)]\n",
    "    n = np.zeros(nrow)\n",
    "#     for i in range(nrow):\n",
    "#         for j in range(nrow):\n",
    "#             if i != j or i==j:\n",
    "#                 comparison = np.sum(evaluator(func_value_matrix[i,], func_value_matrix[j,]))\n",
    "#                 if comparison == ncol:\n",
    "#                     n[j] += 1\n",
    "#                     S[i].append(j)\n",
    "    \n",
    "    for i in range(nrow):\n",
    "        comparison = np.sum(evaluator(func_value_matrix[i,], func_value_matrix[:,]), axis=1)\n",
    "        comparison[i] = 0\n",
    "        indices_dominated = np.where(np.isclose(comparison, ncol))\n",
    "        n[indices_dominated] += 1\n",
    "        S[i] = indices_dominated\n",
    "        \n",
    "    F = []\n",
    "    while True:\n",
    "        Q = np.where(n == 0)[0]\n",
    "        n[Q] -= 1\n",
    "#         for i in range(nrow):\n",
    "#             if n[i] == 0:\n",
    "#                 n[i] -= 1\n",
    "#                 Q.append(i)\n",
    "        if len(Q) == 0:\n",
    "            break\n",
    "        for i in Q:\n",
    "            for j in S[i]:\n",
    "                n[j] -= 1\n",
    "        F.append(Q)\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0694e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_counterfactuals(data, x_obs_nr = None, x_obs = None, figsize=[16,4], save_path = None, save_dir = None,\n",
    "                           moc_file_path = None, n_to_keep = 10, n_to_generate = 10000, add_plausibility_objective = False,\n",
    "                            epsilon_num_percent = 0.005, only_tabGAN_cf = False):\n",
    "    fig, axes = plt.subplots(1,1 if only_tabGAN_cf else 2, figsize = figsize)\n",
    "    if only_tabGAN_cf:\n",
    "        axes = [axes]\n",
    "    if (not x_obs_nr is None) and (not x_obs is None):\n",
    "        raise ValueError(\"Only one of the parameters x_obs_nr or x_obs can be different from zero.\")\n",
    "    elif not x_obs_nr is None: \n",
    "        x_obs = data.loc[x_obs_nr:x_obs_nr,:]\n",
    "    elif not x_obs is None:\n",
    "        x_obs = x_obs\n",
    "    else:\n",
    "        raise ValueError(\"One of the parameters x_obs_nr or x_obs must be different from zero\")\n",
    "    \n",
    "    for ax in axes:\n",
    "        contour_plot = plot_heatmap(ax, incl_colorbar = False, ret_contour = True)\n",
    "        plot_confint_x2_given_x1_and_x3(ax, sigma_x1, sigma_x2)\n",
    "    \n",
    "    if only_tabGAN_cf:\n",
    "        plt.colorbar(contour_plot, ax = ax)\n",
    "    else:\n",
    "        plt.colorbar(contour_plot, ax = axes.ravel().tolist())\n",
    "    \n",
    "    df_cf_tabGAN_toy = tg.generate_counterfactuals(n_to_keep, classifier, x_obs, n_to_generate = n_to_generate,\n",
    "                                                   add_plausibility_objective = add_plausibility_objective,\n",
    "                                                  epsilon_num_percent = epsilon_num_percent)\n",
    "    \n",
    "    axis_titles = [\"tabGAN-cf\"]\n",
    "    axis_datasets = [df_cf_tabGAN_toy]\n",
    "    if not only_tabGAN_cf:\n",
    "        if x_obs_nr is None:\n",
    "            if moc_file_path is None:\n",
    "                raise ValueError(\"if using self-made x_obs, then moc_file_path parameter must be different from None\")\n",
    "            df_cf_moc = pd.read_csv(moc_file_path)\n",
    "        else:\n",
    "            df_cf_moc = pd.read_csv(os.path.join(cf_dir, f\"moc_cf_toy_dataset_obs{x_obs_nr}.csv\"))\n",
    "        if df_cf_moc.shape[0] < n_to_keep:\n",
    "            raise RuntimeError(\"The number of valid counterfactuals generated by moc was fewer than the number requested.\")\n",
    "        axis_titles += [\"MOC\"]\n",
    "        axis_datasets += [df_cf_moc]\n",
    "    legend_categories = set()\n",
    "    for i, df_cf in enumerate(axis_datasets):\n",
    "        axes[i].set_title(axis_titles[i])\n",
    "        categories_unique = np.unique(df_cf[\"x3\"])\n",
    "        legend_categories.update(categories_unique)\n",
    "        for j, cat in enumerate(categories_unique):\n",
    "            df_cf_subset = df_cf[df_cf[\"x3\"] == cat]\n",
    "            axes[i].scatter(df_cf_subset[\"x1\"], df_cf_subset[\"x2\"], color = str(map_x3_to_col(cat)), label = cat)\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.scatter(x_obs[\"x1\"], x_obs[\"x2\"], color = map_x3_to_col(x_obs[\"x3\"]), edgecolor = \"black\",\n",
    "                  zorder=20)\n",
    "        ax.legend()\n",
    "    \n",
    "    if not save_path is None:\n",
    "        if not save_dir is None:\n",
    "            save_path = os.path.join(save_dir, save_path)\n",
    "        plt.savefig(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84543439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_datasets(tgan, dataset_dir, n_synthetic_datasets, n_epochs, subfolder = None,\n",
    "                               n_synthetic_datasets_existing = 0, progress_bar_leave = True):\n",
    "    if not subfolder is None:\n",
    "        dataset_dir = os.path.join(dataset_dir, subfolder) \n",
    "    os.makedirs(dataset_dir, exist_ok = True)\n",
    "    for i in tqdm(range(n_synthetic_datasets_existing, n_synthetic_datasets), desc = \"Generated datasets\",\n",
    "                 leave = progress_bar_leave):\n",
    "        \n",
    "        tgan.train(n_epochs, batch_size = batch_size, restart_training = True, plot_loss = False,\n",
    "                 progress_bar = True, progress_bar_desc = f\"Progress generating dataset {i+1}\")\n",
    "        fake_train = tgan.generate_data()\n",
    "        fake_train.to_csv(os.path.join(dataset_dir, f\"gen{i}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f24ff84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_datasets_for_multiple_epochs(tabGAN, dataset_dir, n_synthetic_datasets, n_epochs_vec,\n",
    "                                                  restart = False, path_finished_epochs_counter = None,\n",
    "                                                  redo_n_epochs_vec = [], plot_only_new_progress = True, **kwargs):\n",
    "    n_epochs_vec = set(n_epochs_vec)\n",
    "    redo_n_epochs_vec = set(redo_n_epochs_vec)\n",
    "    if restart or (path_finished_epochs_counter is None) or (not os.path.exists(path_finished_epochs_counter)):\n",
    "        existing_n_epochs = set()\n",
    "        if not path_finished_epochs_counter is None:\n",
    "            os.makedirs(os.path.dirname(path_finished_epochs_counter), exist_ok = True)\n",
    "    else:\n",
    "        with open(path_finished_epochs_counter, 'rb') as handle:\n",
    "            existing_n_epochs = pickle.load(handle)\n",
    "    n_epochs_new_vec = n_epochs_vec.difference(existing_n_epochs.difference(redo_n_epochs_vec))\n",
    "    \n",
    "    with tqdm(total = len(n_epochs_new_vec) if plot_only_new_progress else len(n_epochs_vec),\n",
    "            desc = \"Epoch subfolder creation\") as pbar:\n",
    "        if not plot_only_new_progress:\n",
    "              pbar.update(len(n_epochs_vec) - len(n_epochs_new_vec))\n",
    "        for i, n_epochs in enumerate(n_epochs_vec):\n",
    "            if n_epochs in existing_n_epochs and not n_epochs in redo_n_epochs_vec:\n",
    "                continue\n",
    "            generate_multiple_datasets(tabGAN, dataset_dir, n_synthetic_datasets, n_epochs = n_epochs,\n",
    "                                       subfolder = f\"Epochs{n_epochs}\", progress_bar_leave = False)\n",
    "            pbar.update(1)\n",
    "            if not path_finished_epochs_counter is None:\n",
    "                existing_n_epochs.add(n_epochs)\n",
    "                with open(path_finished_epochs_counter, 'wb') as handle:\n",
    "                    pickle.dump(existing_n_epochs, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8881368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_datasets_for_multiple_epochs_fast(tabGAN, dataset_dir, n_synthetic_datasets, n_epochs_vec,\n",
    "                                                  restart = False, path_finished_epochs_counter = None,\n",
    "                                                  redo_n_epochs_vec = [], plot_only_new_progress = True,\n",
    "                                                        n_synthetic_datsets_existing = 0, **kwargs):\n",
    "    n_epochs_vec = set(n_epochs_vec)\n",
    "    redo_n_epochs_vec = set(redo_n_epochs_vec)\n",
    "    if restart or (path_finished_epochs_counter is None) or (not os.path.exists(path_finished_epochs_counter)):\n",
    "        existing_n_epochs = set()\n",
    "        if not path_finished_epochs_counter is None:\n",
    "            os.makedirs(os.path.dirname(path_finished_epochs_counter), exist_ok = True)\n",
    "    else:\n",
    "        with open(path_finished_epochs_counter, 'rb') as handle:\n",
    "            existing_n_epochs = pickle.load(handle)\n",
    "    n_epochs_new_vec = n_epochs_vec.difference(existing_n_epochs.difference(redo_n_epochs_vec))\n",
    "    if len(n_epochs_new_vec) == 0:\n",
    "        print(\"All datasets for all epochs are already generated.\")\n",
    "        return\n",
    "    \n",
    "    for j in tqdm(range(n_synthetic_datasets_existing, n_synthetic_datasets), desc = \"Generated datasets\",\n",
    "             leave = True):\n",
    "        with tqdm(total = len(n_epochs_new_vec) if plot_only_new_progress else len(n_epochs_vec),\n",
    "                desc = \"Epoch subfolder creation\", leave=False) as pbar:\n",
    "            if not plot_only_new_progress:\n",
    "                  pbar.update(len(n_epochs_vec) - len(n_epochs_new_vec))\n",
    "            for i, n_epochs in enumerate(n_epochs_new_vec):\n",
    "                epoch_dataset_dir = os.path.join(dataset_dir, f\"Epochs{n_epochs}\")\n",
    "                if j == 0:\n",
    "                    os.makedirs(epoch_dataset_dir, exist_ok = True)\n",
    "                if i == 0:\n",
    "                    restart_training = True\n",
    "                    n_epochs_diff = n_epochs\n",
    "                    last_n_epochs = 0\n",
    "                else:\n",
    "                    restart_training = False\n",
    "                    n_epochs_diff = n_epochs - last_n_epochs\n",
    "                tabGAN.train(n_epochs_diff, batch_size = batch_size, restart_training = restart_training, plot_loss = False,\n",
    "                progress_bar = True, progress_bar_desc = f\"Progress training from epoch {last_n_epochs} to {n_epochs}\")\n",
    "                fake_train = tabGAN.generate_data()\n",
    "                fake_train.to_csv(os.path.join(epoch_dataset_dir, f\"gen{j}.csv\"))\n",
    "                pbar.update(1)\n",
    "                if j == n_synthetic_datasets:\n",
    "                    if not path_finished_epochs_counter is None:\n",
    "                        existing_n_epochs.add(n_epochs)\n",
    "                        with open(path_finished_epochs_counter, 'wb') as handle:\n",
    "                            pickle.dump(existing_n_epochs, handle)\n",
    "                last_n_epochs = n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c3b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_n_epochs_through_prediction(data_train, data_test, dataset_dir, n_epochs_vec, n_synthetic_datasets,\n",
    "                                        save_dir = None, save_path = None, figsize = [14,8], legend_pos=\"best\"):\n",
    "    subfolders = [f\"Epochs{n_epochs}\" for n_epochs in n_epochs_vec]\n",
    "    with tqdm(total=len(subfolders)*n_synthetic_datasets) as pbar:\n",
    "        models = subfolders\n",
    "        result = pd.DataFrame({\"n_epochs\" : models, \"Accuracy\" : 0, \"AUC\" : 0,\n",
    "                              \"SD Accuracy\" : 0, \"SD AUC\" : 0})\n",
    "        accuracy, auc, categories = fit_and_evaluate_xgboost(data_train, data_test, retcats = True)\n",
    "\n",
    "        for i, subfolder in enumerate(subfolders):\n",
    "            accuracy_vec = np.zeros(n_synthetic_datasets)\n",
    "            auc_vec = np.zeros(n_synthetic_datasets)\n",
    "            curr_dataset_dir = os.path.join(dataset_dir, subfolder)\n",
    "            for j in range(n_synthetic_datasets):\n",
    "                path = os.path.join(curr_dataset_dir, f\"gen{j}.csv\")\n",
    "                fake_train = pd.read_csv(path, index_col = 0)\n",
    "                eval_result = fit_and_evaluate_xgboost(fake_train, data_test, categories = categories)\n",
    "                accuracy_vec[j] = eval_result[0]\n",
    "                auc_vec[j] = eval_result[1]\n",
    "                pbar.update(1)\n",
    "            accuracy = np.mean(accuracy_vec)\n",
    "            auc = np.mean(auc_vec)\n",
    "            accuracy_std = np.std(accuracy_vec)\n",
    "            auc_std = np.std(auc_vec)\n",
    "            result.iloc[i,1:] = [accuracy, auc, accuracy_std, auc_std]\n",
    "    fig, ax = plt.subplots(1, figsize = figsize)\n",
    "    color_accuracy = next(ax._get_lines.prop_cycler)['color']\n",
    "    color_auc = next(ax._get_lines.prop_cycler)['color']\n",
    "    plt.plot(n_epochs_vec, result[\"Accuracy\"], label = \"Accuracy\", color = color_accuracy)\n",
    "    plt.scatter(n_epochs_vec, result[\"Accuracy\"], color = color_accuracy)\n",
    "    plt.fill_between(n_epochs_vec, result[\"Accuracy\"] - result[\"SD Accuracy\"],\n",
    "                     result[\"Accuracy\"] + result[\"SD Accuracy\"],\n",
    "                    label = r\"Accuracy $\\pm$ SD Accuracy\", alpha = 0.5, color=color_accuracy)\n",
    "    plt.plot(n_epochs_vec, result[\"AUC\"], label = \"AUC\", color=color_auc)\n",
    "    plt.scatter(n_epochs_vec, result[\"AUC\"], color=color_auc)\n",
    "    plt.fill_between(n_epochs_vec, result[\"AUC\"] - result[\"SD AUC\"], result[\"AUC\"] + result[\"SD AUC\"],\n",
    "                    label = r\"AUC $\\pm$ SD AUC\", alpha = 0.5, color=color_auc)\n",
    "    plt.legend(loc=legend_pos)\n",
    "    if not save_path is None:\n",
    "        if not save_dir is None:\n",
    "            save_path = os.path.join(save_dir, save_path)\n",
    "        fig.savefig(save_path)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d5d6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_datasets_for_multiple_hyperparameters(create_tabGAN_func, hyperparams_vec, n_epochs, dataset_dir,\n",
    "                                                            n_synthetic_datasets,\n",
    "                                                            restart = False, path_finished_hyperparams = None,\n",
    "                                                            redo_hyperparams_vec = [], plot_only_new_progress = True,\n",
    "                                                            hyperparams_name = \"hyperparam\"):\n",
    "    hyperparams_vec = set(hyperparams_vec)\n",
    "    redo_hyperparams_vec = set(redo_hyperparams_vec)\n",
    "    if restart or (path_finished_hyperparams is None) or (not os.path.exists(path_finished_hyperparams)):\n",
    "        existing_hyperparams = set()\n",
    "        if not path_finished_hyperparams is None:\n",
    "            os.makedirs(os.path.dirname(path_finished_hyperparams), exist_ok = True)\n",
    "    else:\n",
    "        with open(path_finished_hyperparams, 'rb') as handle:\n",
    "            existing_hyperparams = pickle.load(handle)\n",
    "    hyperparams_new_vec = hyperparams_vec.difference(existing_hyperparams.difference(redo_hyperparams_vec))\n",
    "    \n",
    "    with tqdm(total = len(hyperparams_new_vec) if plot_only_new_progress else len(hyperparams_vec),\n",
    "            desc = \"Hyperparameters subfolder creation\") as pbar:\n",
    "        if not plot_only_new_progress:\n",
    "              pbar.update(len(hyperparams_vec) - len(hyperparams_new_vec))\n",
    "        for i, hyperparams in enumerate(hyperparams_new_vec):\n",
    "            if isinstance(hyperparams, tuple):\n",
    "                hyperparams_abbreviation = \"\".join(\"_\" + str(s) for s in hyperparams)\n",
    "            else:\n",
    "                hyperparams_abbreviation = \"_\" + str(hyperparams)\n",
    "            tabGAN = create_tabGAN_func(hyperparams)\n",
    "            generate_multiple_datasets(tabGAN, dataset_dir, n_synthetic_datasets, n_epochs = n_epochs,\n",
    "                                       subfolder = \"{}{}\".format(hyperparams_name, hyperparams_abbreviation),\n",
    "                                       progress_bar_leave = False)\n",
    "            pbar.update(1)\n",
    "            if not path_finished_hyperparams is None:\n",
    "                existing_hyperparams.add(hyperparams)\n",
    "                with open(path_finished_hyperparams, 'wb') as handle:\n",
    "                    pickle.dump(existing_hyperparams, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hyperparams_through_prediction(data_train, data_test, dataset_dir, hyperparams_vec, n_synthetic_datasets,\n",
    "                                        save_dir = None, save_path = None, figsize = [14,8], legend_pos=\"best\",\n",
    "                                            plot_sd = True, plot_separate=False,\n",
    "                                           hyperparams_name=\"hyperparam\", x_scale=\"linear\"):\n",
    "    hyperparams_abbreviation_vec = []\n",
    "    for hyperparams in hyperparams_vec:\n",
    "        if isinstance(hyperparams, tuple):\n",
    "            hyperparams_abbreviation_vec.append(\"\".join(\"_\" + str(s) for s in hyperparams))\n",
    "        else:\n",
    "            hyperparams_abbreviation_vec.append(\"_\" + str(hyperparams))\n",
    "    \n",
    "    subfolders = [f\"{hyperparams_name}{hyperparams}\" for hyperparams in hyperparams_abbreviation_vec]\n",
    "    with tqdm(total=len(subfolders)*n_synthetic_datasets) as pbar:\n",
    "        models = subfolders\n",
    "        result = pd.DataFrame({\"n_epochs\" : models, \"Accuracy\" : 0, \"AUC\" : 0,\n",
    "                              \"SD Accuracy\" : 0, \"SD AUC\" : 0})\n",
    "        accuracy, auc, categories = fit_and_evaluate_xgboost(data_train, data_test, retcats = True)\n",
    "\n",
    "        for i, subfolder in enumerate(subfolders):\n",
    "            accuracy_vec = np.zeros(n_synthetic_datasets)\n",
    "            auc_vec = np.zeros(n_synthetic_datasets)\n",
    "            curr_dataset_dir = os.path.join(dataset_dir, subfolder)\n",
    "            for j in range(n_synthetic_datasets):\n",
    "                path = os.path.join(curr_dataset_dir, f\"gen{j}.csv\")\n",
    "                fake_train = pd.read_csv(path, index_col = 0)\n",
    "                eval_result = fit_and_evaluate_xgboost(fake_train, data_test, categories = categories)\n",
    "                accuracy_vec[j] = eval_result[0]\n",
    "                auc_vec[j] = eval_result[1]\n",
    "                pbar.update(1)\n",
    "            accuracy = np.mean(accuracy_vec)\n",
    "            auc = np.mean(auc_vec)\n",
    "            accuracy_std = np.std(accuracy_vec)\n",
    "            auc_std = np.std(auc_vec)\n",
    "            result.iloc[i,1:] = [accuracy, auc, accuracy_std, auc_std]\n",
    "    fig, ax = plt.subplots(1, figsize = figsize)\n",
    "    ax.set_xscale(x_scale)\n",
    "    color_accuracy = next(ax._get_lines.prop_cycler)['color']\n",
    "    color_auc = next(ax._get_lines.prop_cycler)['color']\n",
    "    plt.plot(hyperparams_vec, result[\"Accuracy\"], label = \"Accuracy\", color = color_accuracy)\n",
    "    plt.scatter(hyperparams_vec, result[\"Accuracy\"], color = color_accuracy)\n",
    "    if plot_sd:\n",
    "        plt.fill_between(hyperparams_vec, result[\"Accuracy\"] - result[\"SD Accuracy\"],\n",
    "                         result[\"Accuracy\"] + result[\"SD Accuracy\"],\n",
    "                        label = r\"Accuracy $\\pm$ SD Accuracy\", alpha = 0.5, color=color_accuracy)\n",
    "    if plot_separate:\n",
    "        fig, ax = plt.subplots(1, figsize = figsize)\n",
    "        ax.set_xscale(x_scale)\n",
    "    plt.plot(hyperparams_vec, result[\"AUC\"], label = \"AUC\", color=color_auc)\n",
    "    plt.scatter(hyperparams_vec, result[\"AUC\"], color=color_auc)\n",
    "    if plot_sd:\n",
    "        plt.fill_between(hyperparams_vec, result[\"AUC\"] - result[\"SD AUC\"], result[\"AUC\"] + result[\"SD AUC\"],\n",
    "                        label = r\"AUC $\\pm$ SD AUC\", alpha = 0.5, color=color_auc)\n",
    "    plt.legend(loc=legend_pos)\n",
    "    if not save_path is None:\n",
    "        if not save_dir is None:\n",
    "            save_path = os.path.join(save_dir, save_path)\n",
    "        fig.savefig(save_path)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98d1de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TicTocGenerator():\n",
    "    # Generator that returns time differences\n",
    "    ti = 0           # initial time\n",
    "    tf = time.time() # final time\n",
    "    while True:\n",
    "        ti = tf\n",
    "        tf = time.time()\n",
    "        yield tf-ti # returns the time difference\n",
    "\n",
    "TicToc = TicTocGenerator() # create an instance of the TicTocGen generator\n",
    "\n",
    "# This will be the main function through which we define both tic() and toc()\n",
    "def toc(tempBool=True):\n",
    "    # Prints the time difference yielded by generator instance TicToc\n",
    "    tempTimeInterval = next(TicToc)\n",
    "    if tempBool:\n",
    "        print( \"Elapsed time: %f seconds.\\n\" %tempTimeInterval )\n",
    "\n",
    "def tic():\n",
    "    # Records a time in TicToc, marks the beginning of a time interval\n",
    "    toc(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f60454b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
